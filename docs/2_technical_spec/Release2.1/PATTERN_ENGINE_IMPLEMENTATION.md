# Pattern Engine Implementation Plan

**Release:** 2.1 - Pattern Engine Foundation  
**Status:** Specification  
**Date:** 2025-11-06

---

## âš ï¸ CRITICAL: Cost Optimization Requirement

**MANDATORY IMPLEMENTATION:** Selective context loading for LLM prompts

**Cost Impact:**
- âŒ **Without selective loading:** $17,544/year (100K conversations/month)
- âœ… **With selective loading:** $558/year (100K conversations/month)
- ðŸ’° **Savings: $16,986/year (96.8% token reduction)**

**Token Usage:**
- Full YAML: 9,747 tokens/turn = $0.0015/turn
- Selective loading: 310 tokens/turn = $0.000047/turn
- **31x cost reduction**

**Implementation Rule:**
> **NEVER send full YAML to LLM. Always use selective context extraction.**
> Extract only: behavior goal + template + relevant knowledge + recent history
> Target: ~310 tokens per turn

See `PERFORMANCE_ASSESSMENT.md` for detailed analysis.

---

## Overview

Build production-ready pattern engine that provides:
1. Pattern loading and validation
2. Trigger detection (4 types)
3. Knowledge state tracking
4. Pattern selection by affinity
5. **LLM prompt integration (with selective loading - CRITICAL)**
6. Semantic and behavioral testing

**Full Format Spec:** See `PATTERN_FORMAT.md`  
**Runtime Architecture:** See `PATTERN_RUNTIME_ARCHITECTURE.md`  
**Performance Analysis:** See `PERFORMANCE_ASSESSMENT.md`

---

## Week 1: Core Pattern System

### Goal
Load pattern library and track knowledge state

### Tasks

**1. Pattern Data Structure**
```python
# src/patterns/models.py
@dataclass
class Pattern:
    pattern_id: str
    name: str
    category: str
    trigger: TriggerCondition
    behavior: BehaviorSpec
    updates: KnowledgeUpdates
    tests: TestSpec
    metadata: PatternMetadata
```

**2. Pattern Loader**
```python
# src/patterns/pattern_loader.py
class PatternLoader:
    def load_from_yaml(path: str) -> Pattern
    def load_category(category: str) -> List[Pattern]
    def load_all() -> Dict[str, Pattern]
    def validate_pattern(pattern: Pattern) -> bool
```

**3. Knowledge Tracker**
```python
# src/patterns/knowledge_tracker.py
class KnowledgeTracker:
    user_knowledge: Dict[str, Any]
    system_knowledge: Dict[str, Any]
    
    def update(updates: KnowledgeUpdates)
    def check_prerequisites(pattern: Pattern) -> bool
    def get_state() -> KnowledgeState
    def serialize() -> dict
```

**4. Trigger Detector (Basic)**
```python
# src/patterns/trigger_detector.py
class TriggerDetector:
    def detect_user_explicit(message: str) -> List[str]
    def detect_user_implicit(message: str, context: Context) -> List[str]
    def detect_system_reactive(state_change: StateChange) -> List[str]
    # System-proactive in Week 2
```

### Deliverables
- Pattern data models
- YAML loader with validation
- Knowledge state tracking
- Basic trigger detection
- 15+ unit tests

### Success Criteria
âœ… Load 77 behaviors from YAML  
âœ… Validate pattern structure  
âœ… Track user/system knowledge  
âœ… Detect explicit triggers (keywords)  
âœ… All unit tests pass

---

## Week 2: Behavior Library & Selection

### Goal
Implement pattern selection algorithm and migrate behavior library

### Tasks

**1. Migrate Behavior Library**
```
sandbox/conversation_ux_exercise/atomic_behaviors.yaml
  â†“ Split by category
data/patterns/behaviors/
  â”œâ”€â”€ education.yaml
  â”œâ”€â”€ transparency.yaml
  â”œâ”€â”€ navigation.yaml
  â”œâ”€â”€ assessment.yaml
  â”œâ”€â”€ discovery.yaml
  â”œâ”€â”€ onboarding.yaml
  â”œâ”€â”€ context_extraction.yaml
  â”œâ”€â”€ analysis.yaml
  â”œâ”€â”€ recommendation.yaml
  â””â”€â”€ error_recovery.yaml
```

**2. Add Situation Affinity Scores**
```yaml
# Example: data/patterns/behaviors/education.yaml
- id: B_EXPLAIN_OBJECT_MODEL
  goal: "Teach simplistic object model design"
  situation_affinity:  # NEW
    discovery: 0.3
    assessment: 0.4
    education: 0.9
    clarification: 0.5
  template: |
    Quick note on how this works...
```

**3. Pattern Selector**
```python
# src/patterns/pattern_selector.py
class PatternSelector:
    def __init__(self, patterns: Dict[str, Pattern]):
        self.index = self._build_index(patterns)
    
    def select(
        self,
        triggers: List[str],
        knowledge: KnowledgeState,
        situation: Optional[SituationComposition] = None
    ) -> List[Pattern]:
        # 1. Filter by triggers
        # 2. Filter by prerequisites
        # 3. Score by situation affinity (if provided)
        # 4. Apply priority resolution
        # 5. Return top N
```

**4. LLM Integration**
```python
# src/patterns/llm_integration.py
class PatternPromptBuilder:
    """
    Builds LLM prompts with selective pattern context.
    
    PERFORMANCE TARGET:
    - Full YAML: 9,747 tokens (~$0.0015/turn) âŒ
    - Selective: 310 tokens (~$0.000047/turn) âœ…
    - Reduction: 96.8%
    """
    
    @staticmethod
    def inject_patterns(
        base_prompt: str,
        active_patterns: List[Pattern],
        situation: Optional[Dict[str, float]] = None,
        knowledge: Optional[Dict[str, Any]] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        Inject MINIMAL pattern context into LLM prompt.
        
        CRITICAL: Only include what's needed for response generation.
        Do NOT send full YAML - extract minimal context only.
        
        Target: ~310 tokens per turn
        """
        prompt_parts = [base_prompt]
        
        # Add ONLY the selected pattern's essential context (~50 tokens)
        if active_patterns:
            # Take only first pattern (most relevant)
            pattern = active_patterns[0]
            minimal_context = PatternPromptBuilder._extract_minimal_context(pattern)
            prompt_parts.append(f"\n\nPattern Guidance:\n{minimal_context}")
        
        # Add ONLY relevant knowledge state (~40 tokens)
        if knowledge and active_patterns:
            relevant_knowledge = PatternPromptBuilder._extract_relevant_knowledge(
                knowledge, 
                active_patterns[0]
            )
            prompt_parts.append(f"\n\nRelevant Context:\n{relevant_knowledge}")
        
        # Add ONLY recent conversation history (~150 tokens)
        if conversation_history:
            recent_history = PatternPromptBuilder._format_recent_history(
                conversation_history[-3:]  # Last 3 turns only
            )
            prompt_parts.append(f"\n\nRecent Conversation:\n{recent_history}")
        
        return "\n".join(prompt_parts)
    
    @staticmethod
    def _extract_minimal_context(pattern: Pattern) -> str:
        """
        Extract ONLY essential pattern information.
        
        Target: ~50 tokens
        Include: goal, template, key constraints
        Exclude: metadata, tests, examples, full trigger conditions
        """
        behavior = pattern.behavior
        
        # Minimal context only
        context_parts = [
            f"Goal: {behavior.goal}",
            f"Template: {behavior.template}",
        ]
        
        # Add only critical constraints
        if behavior.constraints:
            if 'max_words' in behavior.constraints:
                context_parts.append(f"Max words: {behavior.constraints['max_words']}")
            if 'tone' in behavior.constraints:
                context_parts.append(f"Tone: {behavior.constraints['tone']}")
        
        return "\n".join(context_parts)
    
    @staticmethod
    def _extract_relevant_knowledge(
        knowledge: Dict[str, Any], 
        pattern: Pattern
    ) -> str:
        """
        Extract ONLY knowledge dimensions relevant to this pattern.
        
        Target: ~40 tokens
        Include: Only dimensions the pattern needs
        Exclude: All other knowledge state
        """
        relevant = []
        
        # Extract only what pattern updates or requires
        if pattern.updates:
            for key in pattern.updates.user_knowledge.keys():
                if key in knowledge.get('user', {}):
                    relevant.append(f"{key}: {knowledge['user'][key]}")
            
            for key in pattern.updates.system_knowledge.keys():
                if key in knowledge.get('system', {}):
                    value = knowledge['system'][key]
                    # Truncate long values
                    if isinstance(value, (list, dict)) and len(str(value)) > 50:
                        relevant.append(f"{key}: [truncated]")
                    else:
                        relevant.append(f"{key}: {value}")
        
        return "\n".join(relevant) if relevant else "No specific context needed"
    
    @staticmethod
    def _format_recent_history(history: List[Dict]) -> str:
        """
        Format ONLY recent conversation turns.
        
        Target: ~150 tokens (3 turns Ã— 50 tokens)
        Include: Last 3 turns only
        Exclude: Full conversation history
        """
        formatted = []
        for turn in history:
            user_msg = turn.get('user', '')[:100]  # Truncate long messages
            assistant_msg = turn.get('assistant', '')[:100]
            formatted.append(f"User: {user_msg}\nAssistant: {assistant_msg}")
        
        return "\n".join(formatted)
```

**5. Complete Trigger Detection**
```python
# src/patterns/trigger_detector.py (enhanced)
class TriggerDetector:
    # Week 1 methods +
    def detect_system_proactive(
        message: str,
        context: Context,
        knowledge: KnowledgeState
    ) -> List[str]:
        # Detect opportunities (timeline, budget mentions)
        # Check if already extracted
        # Return proactive patterns
```

### Deliverables
- 77 behaviors in `data/patterns/behaviors/`
- 40+ triggers in `data/patterns/triggers/`
- Pattern selection algorithm
- LLM prompt integration
- Complete trigger detection
- 20+ integration tests

### Success Criteria
âœ… All 77 behaviors loaded with affinity scores  
âœ… Pattern selection works with/without situation  
âœ… Trigger detection covers all 4 types  
âœ… LLM prompts include pattern context  
âœ… Integration tests pass

---

## Week 3: Testing Infrastructure

### Goal
Enable semantic and behavioral testing for patterns

### Tasks

**1. Semantic Test Framework (LLM-as-Judge)**
```python
# tests/patterns/semantic/test_framework.py
class SemanticTestRunner:
    def __init__(self, llm_client):
        self.llm = llm_client
    
    def run_test(pattern: Pattern, test_case: SemanticTest) -> TestResult:
        # Generate response using pattern
        # Evaluate with LLM-as-judge
        # Return PASS/FAIL with reasoning
```

**2. Behavioral Test Framework (State Assertions)**
```python
# tests/patterns/behavioral/test_framework.py
class BehavioralTestRunner:
    def run_test(pattern: Pattern, test_case: BehavioralTest) -> TestResult:
        # Set up initial state
        # Execute pattern
        # Assert state changes
        # Return PASS/FAIL
```

**3. Integration Test Scenarios**
```python
# tests/patterns/integration/test_scenarios.py
def test_first_time_user_flow():
    # Turn 1: Welcome pattern
    # Turn 2: Output discovery
    # Turn 3: Context identification
    # Assert: Knowledge updated correctly

def test_multi_output_assessment():
    # Assess multiple outputs
    # Switch between them
    # Assert: State tracked per output

def test_error_recovery():
    # User contradicts self
    # System detects contradiction
    # Clarification pattern triggers
    # Assert: Contradiction resolved
```

**4. Pattern Validation Pipeline**
```python
# tests/patterns/validation/validate_patterns.py
def validate_all_patterns():
    # Load all patterns
    # Check structure
    # Verify triggers exist
    # Verify knowledge dimensions exist
    # Check for circular dependencies
    # Validate test coverage
```

**5. CI/CD Integration**
```yaml
# .github/workflows/pattern-tests.yml
name: Pattern Tests
on: [push, pull_request]
jobs:
  test:
    - Run pattern validation
    - Run behavioral tests (fast, free)
    - Run semantic tests (on PR only, costs $)
```

### Deliverables
- Semantic test framework (LLM-as-judge)
- Behavioral test framework (state assertions)
- 5+ integration test scenarios
- Pattern validation pipeline
- CI/CD workflow
- Test documentation

### Success Criteria
âœ… 20+ semantic tests passing  
âœ… 30+ behavioral tests passing  
âœ… 5+ integration scenarios passing  
âœ… Pattern validation catches errors  
âœ… CI/CD pipeline operational  
âœ… Test execution <2 minutes (excluding semantic)

---

## Technical Architecture

### File Structure

```
src/patterns/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ models.py                  # Pattern data structures
â”œâ”€â”€ pattern_loader.py          # YAML â†’ objects
â”œâ”€â”€ pattern_selector.py        # Selection algorithm
â”œâ”€â”€ trigger_detector.py        # 4 trigger types
â”œâ”€â”€ knowledge_tracker.py       # State tracking
â””â”€â”€ llm_integration.py         # Prompt injection

data/patterns/
â”œâ”€â”€ behaviors/
â”‚   â”œâ”€â”€ education.yaml         # 10 behaviors
â”‚   â”œâ”€â”€ transparency.yaml      # 8 behaviors
â”‚   â”œâ”€â”€ navigation.yaml        # 7 behaviors
â”‚   â”œâ”€â”€ assessment.yaml        # 12 behaviors
â”‚   â”œâ”€â”€ discovery.yaml         # 9 behaviors
â”‚   â”œâ”€â”€ onboarding.yaml        # 6 behaviors
â”‚   â”œâ”€â”€ context_extraction.yaml # 8 behaviors
â”‚   â”œâ”€â”€ analysis.yaml          # 6 behaviors
â”‚   â”œâ”€â”€ recommendation.yaml    # 7 behaviors
â”‚   â””â”€â”€ error_recovery.yaml    # 4 behaviors
â”‚
â”œâ”€â”€ triggers/
â”‚   â”œâ”€â”€ user_explicit.yaml     # 15 triggers
â”‚   â”œâ”€â”€ user_implicit.yaml     # 10 triggers
â”‚   â”œâ”€â”€ system_proactive.yaml  # 8 triggers
â”‚   â””â”€â”€ system_reactive.yaml   # 7 triggers
â”‚
â””â”€â”€ knowledge_dimensions.yaml  # User + system knowledge

tests/patterns/
â”œâ”€â”€ semantic/
â”‚   â”œâ”€â”€ test_framework.py
â”‚   â”œâ”€â”€ test_onboarding.py
â”‚   â”œâ”€â”€ test_discovery.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ behavioral/
â”‚   â”œâ”€â”€ test_framework.py
â”‚   â”œâ”€â”€ test_knowledge_updates.py
â”‚   â”œâ”€â”€ test_state_transitions.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_first_time_user.py
â”‚   â”œâ”€â”€ test_multi_output.py
â”‚   â”œâ”€â”€ test_error_recovery.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ validation/
    â””â”€â”€ validate_patterns.py
```

---

## Integration Points

### With Existing System

**1. Session Manager**
```python
# src/core/session_manager.py
class SessionManager:
    def __init__(self):
        self.knowledge_tracker = KnowledgeTracker()  # NEW
        # Remove: self.phase
```

**2. Conversation Orchestrator**
```python
# src/orchestrator/conversation_orchestrator.py
class ConversationOrchestrator:
    def __init__(self):
        self.pattern_selector = PatternSelector(patterns)  # NEW
        self.trigger_detector = TriggerDetector()  # NEW
    
    async def handle_message(self, message: str):
        # Detect triggers
        triggers = self.trigger_detector.detect_all(message, context)
        
        # Select patterns
        patterns = self.pattern_selector.select(
            triggers,
            self.session.knowledge_tracker.get_state()
        )
        
        # Inject into LLM prompt
        prompt = PatternPromptBuilder.inject_patterns(
            base_prompt, patterns
        )
        
        # Generate response
        response = await self.llm.generate(prompt)
        
        # Update knowledge
        self.session.knowledge_tracker.update(patterns[0].updates)
```

---

## Migration from Sandbox

### What to Keep
- Pattern format specification âœ…
- Runtime architecture design âœ…
- UX principles âœ…
- Behavior library (77 behaviors) âœ…
- Trigger library (40+ triggers) âœ…
- Knowledge dimensions âœ…

### What to Implement Fresh
- Pattern loader (new code)
- Pattern selector (new code)
- Trigger detector (new code)
- Knowledge tracker (new code)
- Test frameworks (new code)

### What to Archive
- Sandbox exploration docs (keep for reference)
- Prototype code (not production-ready)
- Design iterations (historical value)

---

## Testing Strategy

### Unit Tests (Fast, Free)
- Pattern loading and validation
- Knowledge state updates
- Trigger detection logic
- Pattern selection algorithm
- **Run on:** Every commit

### Integration Tests (Fast, Free)
- End-to-end conversation flows
- Multi-pattern scenarios
- State transitions
- **Run on:** Every commit

### Semantic Tests (Slow, Costs $)
- LLM-as-judge evaluation
- Response quality checks
- Pattern appropriateness
- **Run on:** PR creation, pre-release

---

## Success Metrics

### Functional
- âœ… 77 behaviors loaded and validated
- âœ… 40+ triggers detected correctly
- âœ… Pattern selection <5ms overhead
- âœ… Knowledge state tracked accurately
- âœ… LLM prompts include pattern context

### Quality
- âœ… 20+ semantic tests passing (>85% accuracy)
- âœ… 30+ behavioral tests passing (100% pass rate)
- âœ… 5+ integration scenarios passing
- âœ… Pattern validation catches structural errors
- âœ… CI/CD pipeline operational

### Performance
- âœ… Pattern index: <100 KB memory
- âœ… Pattern matching: <5ms per turn
- âœ… Knowledge state: <10 KB per session
- âœ… Test execution: <2 min (excluding semantic)

---

## Risk Mitigation

### Risk: Pattern complexity overwhelming
**Mitigation:** Start with 20 core patterns, expand incrementally

### Risk: LLM doesn't follow patterns
**Mitigation:** Semantic tests validate adherence, iterate on prompts

### Risk: Performance overhead
**Mitigation:** Compiled index + lazy loading architecture

### Risk: Test costs too high
**Mitigation:** Cache LLM judge responses, run expensive tests nightly only

---

## Dependencies

**Requires:**
- Release 2 complete (conversation orchestrator, session manager)
- LLM client operational (Gemini)
- YAML parser (PyYAML)

**Enables:**
- Release 2.2 (Situational Awareness)
- Release 2.5 (Semantic Evaluation)

**Blocks:**
- Release 2.2 cannot start without pattern selection
- Release 3 conversation quality depends on patterns

---

## Timeline

**Total Duration:** 3 weeks  
**Effort:** 1 engineer full-time  
**Parallel:** Can run alongside Release 2 completion

**Week 1:** Core system (loading, validation, knowledge tracking)  
**Week 2:** Behavior library & selection (77 behaviors, algorithm)  
**Week 3:** Testing infrastructure (semantic + behavioral + integration)

---

**Owner:** Technical Lead  
**Priority:** High - Blocks Release 2.2 and 2.5  
**Status:** Ready to implement
