# Prescriptive Analytical Assistant - System Architecture Specification

_Figure: High-level architecture of the Prescriptive Analytical Assistant. The Streamlit web UI (gray, right) provides a chat interface for users and displays a detailed log and metrics. The LangChain orchestrator agent (blue, left, running on Cloud Run) handles multi-stage reasoning using a knowledge graph and a vector database. It interacts with Google Vertex AI for language model reasoning and embeddings, and uses Firebase for session persistence._

## Architecture Overview

The Prescriptive Analytical Assistant is a **modular chatbot system** built primarily in Python and designed to run on Google Cloud Platform (GCP). It integrates a structured **knowledge graph**, a **retrieval-augmented generation** (RAG) pipeline, an LLM-based reasoning agent, and a user-friendly frontend. At a high level, the system works as follows:

- **User Interaction**: A business user interacts through a Streamlit single-page web app, asking questions or describing their scenario in natural language. The UI is simple and non-technical, encouraging users to describe business pains or goals without needing AI jargon.
- **Orchestration & Reasoning**: A LangChain-managed agent (following the ReAct paradigm) receives the user input and orchestrates a four-stage conversational flow: **(1)** capture intent and context, **(2)** retrieve relevant knowledge, **(3)** reason with the help of tools/knowledge, and **(4)** generate a final report/proposal. This agent runs within a container (deployed on Cloud Run) and uses Vertex AI's large language model (e.g. PaLM 2/Gemini) for its reasoning and generation steps.
- **Knowledge Base**: Domain knowledge is encoded in a **knowledge graph** built from curated JSON files (AI archetypes, prerequisites, maturity models, etc.). This graph provides a structured representation of AI solution archetypes, business functions, data/prerequisite needs, and so on. The knowledge graph is also augmented with _templated explanations_ (Q&A and narrative text) at each node, which are **embedded into a vector store** for semantic similarity search. This hybrid of graph + vectors enables precise retrieval of information relevant to the user's query[\[1\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=When%20combined%20with%20knowledge%20graphs%2C,specific%20queries).
- **Tools and Memory**: The agent has "tools" at its disposal, such as querying the knowledge graph for relationships or searching the vector database for relevant chunks of text. Throughout the conversation, the agent logs its decisions and intermediate results. A short-term memory within the session retains context (e.g. earlier user inputs, chosen archetypes) so the agent can handle follow-up questions or iterative refinement.
- **Outputs**: The final output is a **prescriptive report** for the user - typically an executive-style summary of an AI project idea (or set of ideas) tailored to the user's business context. It includes rationale, feasibility (checking prerequisites against the organization's maturity), and next-step recommendations. The system also produces **technical logs and evaluation metrics** that are visible in the UI's log panel for transparency.

This architecture aligns with recent best practices in combining knowledge graphs with RAG to improve retrieval quality and answer accuracy[\[1\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=When%20combined%20with%20knowledge%20graphs%2C,specific%20queries). All components are chosen for **low cost or free-tier availability**, making the solution cost-effective to deploy and scalable for demonstration purposes. Below, we detail each subsystem of the architecture.

## Knowledge Graph Pipeline

The foundation of the system's knowledge is a **knowledge graph** constructed from structured JSON files (e.g., AI_archetypes.json, AI_prerequisites.json, AI_discovery.json). We convert this JSON knowledge into a graph of nodes and relationships to enable structured traversal and reasoning. Key design points include:

- **Node Definitions**: Each key concept becomes a node in the graph. For example, each _AI use-case archetype_ (e.g. "Anomaly Detection", "Optimization & Scheduling") is a node with properties like description, typical models, example outputs, etc. Likewise, business functions (e.g. "Manufacturing", "Marketing"), problem/pain types (e.g. "Cost pressure", "Customer churn"), data/prerequisite types, and maturity stages are nodes in the graph. We attach descriptive text from the JSON as node attributes (so the node "Anomaly Detection" carries its definition, typical models, and so on).
- **Edge Relationships**: We add edges to represent meaningful relations:
- _Pain → Archetype_: We map each problem or pain point to one or more AI solution archetypes that can address it. (For example, a pain point of **"efficiency loss"** might connect to the **"Optimization & Scheduling"** archetype[\[2\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Include).) This mapping ensures the agent can traverse from a user's problem to relevant solution ideas.
- _Function/Industry → Archetype_: We connect business functions or sectors to archetypes commonly applied in that context. For instance, a "Supply Chain" function node might link to forecasting or optimization archetype nodes if those are often used in supply chain scenarios.
- _Archetype → Prerequisite/Data_: Each archetype node connects to prerequisite nodes that denote required **data types**, **technical capabilities**, or **infrastructure**. For example, a "Predictive Maintenance" archetype might link to "Sensor/IoT data" and "Time-series data pipeline" prerequisites. Similarly, archetypes that involve deep learning might link to prerequisites like "ML Engineers" (expertise) or "GPU infrastructure".
- _Maturity → Archetype Constraints_: We encode organizational maturity considerations either as special edges or node attributes. For instance, archetypes can be tagged or linked with the minimum maturity level required (e.g., complex prescriptive archetypes might be tagged as requiring a higher maturity stage). This allows filtering recommendations by the user's readiness. (E.g., an **"Exploring"** maturity node might only connect to foundational archetypes like basic analytics, whereas **"Operationalized"** maturity might connect to all archetypes including advanced AI[\[3\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20complex%20deep%20learning).)
- **Implementation**: We use **NetworkX** (a Python graph library) to build and manage the graph in-memory. NetworkX allows dynamic addition of nodes/edges and straightforward traversal algorithms in Python code. Each node can have a dictionary of attributes (we store descriptions, examples, etc. as attributes)[\[4\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=Subsequently%2C%20we%20construct%20the%20Knowledge,to%20form%20a%20comprehensive%20representation). This choice keeps the setup simple-no separate graph database server is needed for the MVP. If needed in the future, we can export or synchronize this graph to Neo4j and use Cypher queries, but for now NetworkX provides the needed functionality for a free-tier solution.
- **Traversal & Querying**: The graph enables **structured queries** during the agent's reasoning. For example, given a pain point identified in conversation, the agent can traverse to connected archetype nodes to see what solutions are relevant. From an archetype node, it can hop to prerequisite nodes to gather what's required to implement that solution. This structured knowledge is crucial for the agent's chain-of-thought reasoning, ensuring it doesn't rely solely on the LLM's parametric knowledge. By traversing the graph, the agent can ground its reasoning in the curated relationships (e.g., _"Pain X is linked to Archetype Y; Archetype Y requires data Z - does the user have data Z?"_).
- **Graph Updates**: The graph is built at initialization from static JSON, but it's flexible. During a session or as knowledge grows, we could dynamically update nodes or edges (for example, if a new archetype is learned, or user-specific information is added as nodes). Because it's in-memory, such updates reflect immediately for that session (and could be persisted by updating the JSON or using Neo4j for a long-term store).

By structuring domain knowledge as a graph, we enable **cross-modal reasoning** and precise knowledge retrieval. The knowledge graph adds an explainable backbone to the chatbot: the assistant can trace _why_ it recommended a project (following edges from user's situation to an archetype and prerequisites), rather than treating the knowledge base as a black box. This approach follows the principle that knowledge graphs provide an interconnected framework that improves embedding quality and retrieval efficiency in RAG systems[\[1\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=When%20combined%20with%20knowledge%20graphs%2C,specific%20queries).

## RAG Integration (Knowledge Retrieval)

To augment the language model with factual knowledge, the system employs a **Retrieval-Augmented Generation (RAG)** architecture. All relevant information from the knowledge graph is pre-processed into text chunks, embedded, and indexed for similarity search. The RAG design has these components:

- **Curated Knowledge Chunks**: We generate textual **QA pairs or narrative snippets** for each important node in the knowledge graph. Rather than simply storing raw JSON, we create human-readable summaries. For example:
- For an **Archetype node** like "Optimization & Scheduling", a stored chunk might be: _"Q:_ _What is Optimization & Scheduling?_ _A:_ _It's a prescriptive AI archetype that finds the best combination of actions/resources to maximize an objective. It addresses efficiency losses by optimizing schedules or resource allocations. Common techniques include linear programming and reinforcement learning. Example outputs: optimal shift plans, delivery routes. Requires historical process data and optimization expertise."_ This chunk combines definition, problem addressed, techniques, and prerequisites in a narrative form.
- For a **Prerequisite node** like "Structured tabular data", the chunk could be a definition and why it matters (from the JSON description) and maybe what AI methods depend on it[\[5\]](file://file-NmJA3Jy2vq4bTEeF5cFCDL#:~:text=,Decision%20Tree)[\[6\]](file://file-NmJA3Jy2vq4bTEeF5cFCDL#:~:text=,Market%20basket%20rules).
- We do this for pain types, maturity levels (e.g., what does "Piloting" stage mean), etc., as needed. These templated explanations ensure that when the assistant retrieves information, it gets _contextualized, ready-to-use prose_ rather than just raw terms.
- **Semantic Embeddings**: Each text chunk is then converted into a vector embedding using a **semantic embedding model**. We leverage **Vertex AI's embedding API** (for example, the textembedding-gecko model) to obtain high-dimensional vector representations of each chunk. This model yields 768-dimensional embeddings that capture semantic meaning of the text. By using Vertex, we avoid hosting our own embedding model and can embed all chunks efficiently via API. (Alternatively, we could use an open-source model like SentenceTransformers in local mode, but Vertex's managed model is optimized and easy to call.)
- **Vector Database**: All embedding vectors are stored in a **vector index** to enable fast similarity search. For the MVP, we favor free or open-source options:
- **FAISS** (Facebook AI Similarity Search) is an in-memory vector index library we can use within the Python app. FAISS can handle hundreds of thousands of vectors easily on a single instance. It provides nearest-neighbor search for embeddings with minimal latency and no external dependencies (great for local or Cloud Run deployment).
- **Pinecone** (managed vector DB) is an alternative if we needed persistent storage of vectors across sessions or larger scale. Notably, Pinecone offers a free tier supporting up to ~100k vectors, which could cover our needs[\[7\]](https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df#:~:text=,to%20a%20more%20scalable%20solution). For now, we can run with FAISS to keep everything within free compute; switching to Pinecone would just require using their SDK and providing an API key.
- **Indexing and Chunking**: We carefully index the knowledge:
- Each chunk is associated with metadata (like the node ID or type it came from). This way, when we retrieve a chunk, we know which knowledge graph node it corresponds to (useful for follow-up logic such as traversing neighbors).
- Chunk size is kept moderate (a few sentences each) to ensure that retrieval is _specific_. The content is granular enough that a query won't accidentally pull in an entire page of unrelated info. (For long documents, we'd use a text splitter[\[8\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=2), but our curated knowledge is already concise.)
- We ensure important keywords are present in the chunks so that semantic search and even keyword search are effective. For instance, the archetype chunk will explicitly mention problems it solves and required data, to connect with user queries on those topics.
- **Similarity Search Workflow**: During conversation (specifically in Stage 2 of the agent flow), the user's query or the derived _goal statement_ is turned into a query vector (using the same embedding model). The vector database is queried for the top-\$N\$ similar chunks. For example, if the user asks about "improving inventory turnover and we're a small manufacturing firm," the retrieval might return chunks about _Optimization & Scheduling_ (due to "inventory" and "efficiency" context), about _Manufacturing / Production_ function (tools like MES mentioned), and maybe about _Data Maturity_ if the query hinted at data issues. We typically take the top 3-5 chunks.
- **Grounding and Deduplication**: The retrieved snippets are then filtered and combined. We remove any obviously redundant results (ensuring we don't feed the same fact twice) and verify they cover diverse angles (ideally one archetype, one function-specific insight, one prerequisite, etc.). This set of **curated facts & constraints** becomes the grounding context for the LLM[\[9\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,Final%20formatted%20report%2C%20checklist). For example, the agent might gather facts like: _"Optimization & Scheduling uses linear programming to improve efficiency"_, _"Manufacturing uses MES systems for production tracking"_, _"SME in exploring stage has limited AI governance"_ - each coming from a different piece of the knowledge base.
- **Usage in Generation**: These retrieved pieces are injected into the LLM's context for Stage 3 reasoning. In practice, we might concatenate them into the prompt (perhaps prefaced like "Here are some relevant facts:\\n- Fact1...\\n- Fact2..." etc.) or store them such that the agent can call on them as needed. This _retrieval augmentation_ ensures the LLM's answers are **grounded in the factual knowledge graph content** rather than purely from its training memory. All claims the assistant makes (like recommending a specific AI method or noting a prerequisite) can thus be traced to a source chunk.

Overall, the RAG integration is crucial for **domain specificity** - it tailors a general LLM (like PaLM/Gemini) to speak with knowledge of SME pain points, archetypal solutions, and AI feasibility constraints. Vectorization of the knowledge base is "foundational" to this approach[\[10\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Force%20explicit%20mapping). By preparing semantically-indexed knowledge, the agent can retrieve information on-the-fly that is directly relevant to the user's query, significantly improving relevance and correctness. This architecture follows recommended practice to use robust document indexing and embedding for any non-trivial knowledge assistant[\[10\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Force%20explicit%20mapping).

## Orchestration with LangChain and ReAct

We implement the multi-step reasoning process using **LangChain** to orchestrate the LLM and tools, following a ReAct (Reason+Act) agent approach. The conversation flow is explicitly divided into stages, each with a specific role, as outlined in the blueprint[\[11\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C,Multi%E2%80%91hop%20reasoning%20%26%20agentic)[\[12\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,report%2C%20checklist%2C%20evaluation%20form):

- **Stage 1: Intent & Context Capture** - The assistant first interprets the user's query to understand the high-level intent and context. It identifies key pieces of information such as the business **function/domain** involved, the core **problem or pain type**, and the organization's **maturity level** (if mentioned by the user). The output of this stage is a clear **goal statement** or structured representation of the query, which will guide subsequent steps[\[11\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C,Multi%E2%80%91hop%20reasoning%20%26%20agentic).
- _Implementation:_ We use a short **LLM prompt** or classification chain to extract these details. For example, we might prompt: _"Identify the business function, pain point, and AI readiness from the user query."_ The LLM (Vertex model) can return a structure like _{function: Manufacturing, pain: "yield loss/inefficiency", maturity: "Piloting"}_. If information is missing or ambiguous, the agent may ask a **clarifying question** from the user (this is logged and counted as an ambiguity resolution moment). We leverage LangChain's facilities to do this in a single-turn if possible, or multi-turn if clarification is needed. By the end of Stage 1, we have the user's need translated into an internal representation (for example: "User is looking for ways to reduce **efficiency loss** in **Manufacturing**, and their org is at **Piloting** stage").
- _Why important:_ This routing step ensures the next phases are targeted. It mirrors how an innovation consultant would first clarify what the client is looking for and their context. (In other words, _intent resolution_ is treated as a first-class task, not left implicit.)
- **Stage 2: Retrieval & Grounding** - Given the structured intent from Stage 1, the agent now performs **information retrieval** to ground its knowledge. This corresponds to the RAG step described earlier. We take the salient pieces (function, pain, maturity) and either form a search query or directly use the goal statement to query the vector database of knowledge chunks. The agent retrieves the most relevant knowledge snippets (archetypes, facts, constraints) from the vector store[\[9\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,Final%20formatted%20report%2C%20checklist). It may also do specific graph traversals - e.g., find all archetypes connected to the identified pain node, or check what prerequisites are linked to those archetypes.
- The result of Stage 2 is a collection of **curated facts** that will inform the solution. We can think of this as the agent building its "evidence deck" before making a recommendation. In practice, LangChain can execute a **RetrievalQA** chain here: the user's context is used to fetch docs from the vector index. We augment that by also using direct graph-based lookup for any obvious mappings (for example, if pain = "churn", directly fetch the "Churn Modeling" archetype node content).
- These retrieved facts are stored in the agent's memory (or passed along explicitly). For example, the agent might now have:
  - Fact 1: "Optimization & Scheduling is a prescriptive AI technique for efficiency improvement[\[2\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Include)."
  - Fact 2: "Manufacturing operations often use MES (Manufacturing Execution Systems) for tracking production[\[13\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,)."
  - Fact 3: "Your maturity level is Piloting, meaning small prototypes exist but AI is not widespread."
  - Fact 4: "Prerequisites for optimization projects include historical process data and possibly an optimization specialist."
- These facts form a grounded context that the _next stage's reasoning will refer to_. They mitigate hallucination and keep the agent focused on feasible, relevant solutions.
- **Stage 3: Reasoning & Synthesis** - This is the core analytical step where the agent uses the LLM to reason over the retrieved facts and the user's goal, ultimately synthesizing a prescriptive solution. We implement this as a **ReAct loop** orchestrated by LangChain: the agent will alternate between **"thinking" (reasoning)** and **"acting" (invoking tools)** until it arrives at a solution[\[14\]](https://docs.langchain.com/oss/python/langchain/agents#:~:text=Tool%20use%20in%20the%20ReAct,loop). In our case, the primary tool results (the knowledge retrieval) are already in hand, but the agent can still call additional tools if needed (for instance, a tool to query the knowledge graph for more detail on a particular archetype, or a calculator if some ROI estimate is needed).
- We set up the LLM with a **role prompt** that establishes the context. For example: _"You are a_ _Senior Innovation Strategist_ _specializing in AI solutions for business problems_[_\[15\]_](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,data%20sources%2C%20deduplicate%20retrieval%2C%20and)_. You have access to factual knowledge about AI use-cases. Your goal is to propose a tailored solution maximizing ROI within the client's capabilities."_ This gives the agent a persona and objective.
- We also explicitly prompt the **chain-of-thought scaffold** we expect: the agent should reason through the sequence _Pain point → Relevant archetypes → Maturity constraints → Feasible AI solution → Prescriptive recommendation_[\[16\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=feasibility%20for%20high%E2%80%91growth%20organizations.%E2%80%9D%20,claims%20to%20the%20structured%20store). This can be encouraged by few-shot examples or instructions (effectively telling the model to follow a stepwise approach).
- **ReAct orchestration:** Using LangChain's agent tools, the LLM will output "thoughts" and possibly tool actions. For example, a reasoning step might be: _"The user's pain is efficiency loss in manufacturing. Possibly relevant archetypes: optimization, predictive maintenance. Let's see prerequisites for each."_ Then it might invoke a graph_tool("get_prerequisites('Optimization & Scheduling')") which we implement to query the graph. The tool might return: _"Prerequisites: requires historical process data; optimization algorithm expertise; relevant infrastructure."_ The agent evaluates this relative to the user's Piloting maturity (maybe thinking _"They might have some data but not advanced optimization experts."_). It could similarly check another archetype. Through these steps, the agent converges on a recommendation that fits the user (e.g., _"Optimization & Scheduling" is viable if data exists and they partner with an expert, else maybe a simpler approach."_). Finally, the agent decides on the best solution path.
- **Multi-hop reasoning** is supported: the agent can chain together insights (possibly using the retrieved facts as intermediate observations). LangChain's ReAct framework will feed each tool's output back into the model's context for the next reasoning step[\[14\]](https://docs.langchain.com/oss/python/langchain/agents#:~:text=Tool%20use%20in%20the%20ReAct,loop), so the agent incrementally builds the solution. All these internal steps are logged (and can be shown in the log panel).
- The end of Stage 3 is the agent producing a **draft proposal or answer**. This draft will include the chosen solution (or top 1-2 solutions) with reasoning. For example, it might be an internal representation or just a text: _"Draft: Recommend implementing an_ _Optimization & Scheduling_ _pilot for production planning to reduce inefficiency. Rationale: aligns with efficiency loss pain; uses historical production data; organization can support this in Piloting stage. Also provide a data readiness checklist and ROI estimate."_ This is not yet shown to the user verbatim, it's an intermediate result.
- Importantly, throughout Stage 3 we enforce **explainability**. The agent is prompted to be explicit about _why_ it recommends something, referencing the facts (this will later appear in the final output as rationale and footnotes if needed). For every claim about the solution, the agent is instructed to ensure it's grounded in the retrieved knowledge or a logical inference thereof[\[17\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,claims%20to%20the%20structured%20store). Any assumptions or unknowns (like if the availability of data is uncertain) are to be noted. This is aligned with the blueprint's guidance to make outputs transparent and to surface assumptions[\[17\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,claims%20to%20the%20structured%20store).
- **Stage 4: Report Generation & Formatting** - In the final stage, the system takes the draft solution content and produces a polished, user-friendly report or response. This involves formatting the content, adding any required sections, and ensuring the tone is appropriate for a non-technical user (e.g., an executive).
- We use either a templating approach or an additional LLM prompt to generate the final formatted output. One approach is to prepare **template sections** such as:
  - _Executive Summary_: one or two paragraphs summarizing the recommendation and expected benefit (e.g., _"We propose an AI-based Optimization & Scheduling project to cut production idle time by ~15%, improving operational efficiency."_).
  - _Feasibility & Prerequisites_: a checklist or paragraph noting what is required (data needed, expertise, timeline considerations), calibrated to the user's maturity (e.g., _"Since your AI maturity is Piloting, ensure you have at least one data engineer to set up data pipelines, and start with a small-scale pilot."_).
  - _Projected Impact_: if possible, a rough ROI or impact estimate, or at least the key KPIs it will improve (this could be generated via a small reasoning or formula, possibly using a rule of thumb).
  - _Next Steps_: a short list of recommended next steps to get started on the project.
  - _References/Explanations_: if this were a formal report, we could include an appendix with the evidence and reasoning (for demo purposes, we might omit or keep it high-level).
- The LLM can be prompted with a format like: _"Given the draft proposal and context, produce a structured output with the following sections: ..."_. We provide the draft from Stage 3 as input, along with a system message that includes any specific formatting rules (for instance, "present any checklist as a bullet list"). Since Stage 3 already has most of the content, Stage 4 is mostly about **rephrasing and organizing** it into a clean presentation.
- We ensure the final tone is **prescriptive but not overly technical**, as per the conceptual blueprint - i.e., it focuses on actionable recommendations ("what to do and why"), rather than lengthy technical detail. Any deep technical justifications remain in the background log or can be offered if the user asks.
- The output of Stage 4 is the **message sent to the user in the chat interface**.

LangChain facilitates this orchestration by letting us define chains and agents for each stage and then combining them. We likely implement Stage 1 and Stage 4 as straightforward LLM chain calls (with specific prompts), while Stage 2 is a retrieval call, and Stage 3 is a custom **ReAct agent**. The ReAct agent uses the tools we define (like a GraphQueryTool, the vector store retrieval tool, etc.) and alternates reasoning and acting until it outputs the final draft[\[14\]](https://docs.langchain.com/oss/python/langchain/agents#:~:text=Tool%20use%20in%20the%20ReAct,loop). The framework's callback system will allow us to capture each of these steps for logging (see Observability).

By structuring the reasoning into these stages, we address the complexity of the task in manageable steps[\[11\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C,Multi%E2%80%91hop%20reasoning%20%26%20agentic). This design also matches known multi-stage agent patterns for complex query answering, where early stages gather context and later stages perform reasoning[\[11\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C,Multi%E2%80%91hop%20reasoning%20%26%20agentic). It ensures that the final answer is both **relevant** (thanks to grounding in Stage 2) and **coherent** (thanks to careful reasoning in Stage 3). It's a practical implementation of the recommended agentic workflow for this domain, where the LLM must juggle knowledge of pains, archetypes, maturity, etc., in a controlled manner.

## Frontend Design

The user interface is a **single-page Streamlit app** that provides an accessible chat experience while also exposing technical details and metrics to those interested. The frontend is composed of three main elements:

- **Chat Interface**: This occupies the main area of the page. It consists of a chat history display and an input box for the user. The design is akin to familiar messaging apps:
- User messages are displayed (e.g., as chat bubbles or text blocks on the right), and assistant responses are displayed on the left. We ensure each assistant response is formatted in Markdown (since Streamlit supports Markdown) so that sections, bullet points, or bold highlights appear nicely in the chat.
- The input box allows the user to type their query or follow-up questions. We might label it with a prompt like "Describe your business problem or ask a question…". When the user hits enter, the query is sent to the backend agent for processing.
- The chat interface is kept **non-technical** and conversational. The assistant's tone here is friendly and executive-oriented. For example, it will say "Based on what you've told me, a possible solution is…" rather than "Vector DB search results indicate…". The goal is to keep business users engaged and not overwhelmed. The deep technical reasoning is happening behind the scenes, not in the chat responses (unless the user specifically asks for it).
- Streamlit's simplicity means the chat can be updated by simply appending to a list of messages in state and re-rendering. The app will likely use st.chat_message or similar high-level chat components if available, or a simple st.markdown for each message with distinctive formatting.
- **Log Panel (Technical Trace)**: A secondary panel (possibly in a sidebar or an expandable section) shows the **detailed log of the agent's internal reasoning and tool use**. This is primarily for **transparency and debugging** and serves the "deep technical detail" requirement:
- The log panel will list each significant step the agent took: e.g., "Parsed user intent: function=Manufacturing, pain=Cost, maturity=Exploring", "Retrieved 3 knowledge chunks (Optimization, Manufacturing tools, Data maturity)…", "Tool call: GraphQuery-> Prerequisites for Optimization", "LLM reasoning: considering solution X…", etc. Essentially, it's a trace of Stage 1 through Stage 4 as it happened, including intermediate thoughts. LangChain's ReAct agent naturally produces a trace (thoughts and actions), which we can capture.
- We will use **color-coding or visual cues** to distinguish different types of log entries. For instance:
  - LLM "thought" messages might be shown in _italics_ or a certain color (e.g., blue text) preceded by "🤔".
  - Tool invocations could be in green and prefixed by a tool icon or keyword.
  - Tool results (observations) might be indented or shown in a different color.
  - Warnings or errors (if any) could be in red.
  - Stage transitions could be bolded (e.g., "**Stage 2: Retrieval**" as a separator).
- The log is _verbose by design_ - it may include raw data like similarity scores or the exact content of retrieved chunks. This can appear "intimidating" to laypersons (which is why it's hidden by default), but it demonstrates the thoroughness of the system to evaluators. A CTO or an AI engineer evaluating the demo can open this panel and see that the assistant is not hallucinating answers but is referencing known data and reasoning step-by-step.
- Technically, in Streamlit we can implement this log panel as an st.expander("Show Technical Log") which, when expanded, reveals the formatted log. Each refresh of the app (each user interaction) will update the log content. We ensure the text is formatted with Markdown or styled text to highlight the different elements.
- **Metrics Snapshot Panel**: We also display evaluation metrics about the conversation's usefulness in the UI. This could be a small section in the sidebar or just below the chat.
- The metrics might be shown as a **percentage score** (e.g., "Conversation Usefulness: 80%") or as a visual progress bar / gauge. Streamlit could use st.progress or a custom matplotlib gauge to illustrate this.
- We might also break it down into sub-metrics, for example using colored checkmarks or badges for each criterion (ambiguity resolved, relevant solution found, prerequisites covered, etc.). For instance: a checklist like:
  - ✅ Ambiguity Resolved
  - ✅ Relevant Archetype Identified
  - ⚠️ Prerequisites Partially Covered
  - ✅ Proposal Generated
  - ⚠️ Maturity Alignment OK (maybe partially okay) This gives a quick glance at how the assistant did.
- The metrics are updated at key points. Likely, after the assistant formulates the final answer for the user's main query, we compute the metrics and display them. If the conversation continues (multi-turn), we could update the metrics again after each resolved query.
- The purpose of showing metrics to the user can be twofold: (1) to instill confidence that the assistant is being evaluated (which subtly indicates quality), and (2) to allow the user to see if anything was potentially missing (for example, if "Prerequisites" is not checked, the user might ask "What prerequisites do I need?" as a follow-up).
- We will present these metrics in a user-friendly way, perhaps with a short explanatory note (e.g., "Score 80% - The assistant identified a relevant solution and provided a plan. Some prerequisite considerations might need more detail."). However, we won't overwhelm with technical jargon - it will be plain language.

The frontend is designed to **keep the primary interaction simple and hide complexity unless asked**. By default, a business user can just chat and read the assistant's recommendations. For more curious or technical users (or during demonstrations), the log and metrics provide insight into the "AI's thought process" and the thoroughness of the solution. This duality addresses the blueprint's note about _showing deep knowledge without scaring users_[\[18\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=1,Transparency%20%26%20Precision) - the depth is there, but tucked into a log panel rather than the main conversation, achieving **transparency with opt-in detail**.

From a development standpoint, using Streamlit allows rapid UI development entirely in Python. It also easily integrates with the Python backend (no complex web frontend needed). This choice aligns with the need for a quick, interactive demo that can be run locally or in the cloud.

## Observability Layer

Observability is a key non-functional aspect of this system - we want to **log every significant action and decision** the assistant makes for debugging and transparency. In this MVP, the observability layer is implemented as in-memory logging with clear visualization, rather than a full external monitoring stack (to stay within free tier and demo scope):

- **Comprehensive Logging**: The system logs **all tool uses, intermediate decisions, and data flow** between stages. This includes:
- When Stage 1 parses the user query, we log the extracted intent (and the prompt that was used to get it, if needed).
- All retrieved knowledge chunks in Stage 2 are logged (with maybe their similarity scores or which node they came from).
- Each step of the Stage 3 ReAct reasoning is logged: the model's "thought" text, any tool it decides to call (and the arguments), and the results returned from the tool. For example, if the agent thinks "I should look up relevant archetypes" and calls a search, we log that thought and the act.
- Threshold decisions or condition checks are logged. For instance, if we have a rule "only proceed if at least one relevant archetype was found", we would log whether that condition was met. Or if the agent checks maturity alignment (e.g., "Recommending deep learning project? No, user maturity is too low"), we log that decision point.
- Data passed between stages (like the final list of facts from Stage 2 to Stage 3) is also recorded in the log. Essentially, any hand-off is recorded so we can reconstruct exactly what information the LLM saw and on what basis it made a decision.
- Errors or exceptions (should they occur) are caught and logged with stack traces or messages. For example, if the vector DB lookup fails or a tool raises an error, it will appear in the log (and the agent will handle it gracefully).
- **In-Memory Storage**: The logs are stored in a Python list or similar structure during the session. We do not write these logs to disk or external storage by default (to avoid any cost or privacy issues with long-term storage). This means logs persist as long as the Streamlit session is active. If the app resets or the session ends, logs are cleared. This is acceptable for demonstration since their main utility is real-time transparency.
- **Rendering for Demo**: As described in the Frontend section, the logs are rendered in the UI with formatting. We take care to preserve the sequence and nesting of actions:
- One technique is to indent tool outputs under the thoughts that prompted them (mimicking a tree or step-by-step transcript). We could use markdown blockquotes or HTML/CSS (if allowed in Streamlit) for indentation and color spans for different types of log lines.
- We label each entry with timestamps or step numbers if needed to show the progression.
- Because we want this for demo clarity, we may simplify the raw LangChain trace (which can be verbose) into a more human-readable form. For example, instead of showing the raw JSON of a tool call, we output a sentence: "**Tool**: GraphQuery('Optimization & Scheduling prerequisites') -> returned: \['historical data', 'optimization expert'\]".
- **No PII / Sensitive Data**: The knowledge base doesn't include personal data, and user queries are expected to be about business problems in general. But if any sensitive info were present, our logs would capture it. Since we're not persisting logs long-term, this mitigates some risk. However, we ensure not to accidentally expose any secret keys or credentials in the log. For instance, when calling Vertex AI, we won't log the API key. We only log the content and decisions.
- **Utility for Evaluation**: These logs are incredibly useful for evaluators and developers. A reviewer can read through and verify that at point X, the agent used Fact Y from the knowledge base to justify its recommendation. This is essentially a built-in **explainability feature**. It addresses the need for _decision explanation appendices_ and verifying evidence[\[19\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20Y%3B%20evidence%20and%20thresholds) - rather than only providing an explanation in final output, we provide the actual decision trace live.
- **Extensibility**: If this were to progress beyond demo, we could integrate with proper observability tools:
- For example, we could send logs to **Google Cloud Logging (Stackdriver)** for persistent storage and off-line analysis.
- We could set up monitoring alerts if the agent enters an error state or if certain metrics drop (though not in MVP scope).
- We might also log user interactions (anonymized) to Firebase or BigQuery if we wanted to analyze how users are using the system. But again, MVP focuses on the real-time aspect.

In summary, the observability layer ensures the system is not a black box. Every reasoning step is traceable. This is crucial in agent-based AI systems for trust and debugging[\[20\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=ethical%20considerations). By keeping logs in-memory and displaying them, we meet the demo goals without incurring extra cost or complexity. Observability is designed in (not an afterthought), consistent with the idea that we _"map out every step of the agent's workflow"_ and have insight into each decision[\[21\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=Map%20out%20every%20potential%20step,step%20problem).

## Evaluation Metrics

We implement a lightweight **conversation evaluation module** to score the assistant's performance on each session. The focus is on **usefulness** and whether the assistant accomplished key objectives in guiding the user. Our approach uses a **rule-based rubric**, aligned with the project's goals, rather than generic NLP metrics. Here's how it works:

- **Key Evaluation Criteria**: Based on the project requirements, we define several yes/no or scaled questions that the conversation should ideally satisfy:
- **Ambiguity Resolution** - Did the assistant actively resolve unclear or ambiguous inputs? For instance, if the user's problem description was vague, did the assistant ask clarifying questions and successfully pinpoint the real need?
- **Pain-Point to Archetype Mapping** - Did the assistant correctly identify an AI solution archetype (or approach) that addresses the user's pain point? In other words, was a relevant AI use-case recommended for the given problem? (e.g., user mentions cost inefficiency, assistant zeroes in on an optimization solution)[\[2\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Include).
- **Prerequisite Surfacing** - Did the assistant bring up the necessary prerequisites or readiness factors for the proposed solution? This could be mentioning data needs, technical expertise required, or infrastructure considerations. We expect a useful assistant to not only propose an idea but also mention what's needed to execute it (fulfilling the "readiness checklist" aspect)[\[22\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Agentic).
- **Maturity Alignment** - Was the recommendation aligned with the user's maturity stage? We check if the solution is neither too advanced nor too simplistic given the organization's AI maturity. For example, if the user is just exploring AI, the assistant should not recommend a complex deep learning project that needs a fully mature data pipeline (that would score negatively)[\[3\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20complex%20deep%20learning). If the recommendation matches the maturity (e.g., a foundational pilot for an exploring company), that's a positive.
- **Solution Concreteness (Proposal Quality)** - Did the assistant deliver a _concrete proposal_ or next step, rather than generic advice? A valid proposal might be a specific project suggestion or action plan (with some detail), as opposed to vague statements. We want the conversation to end with actionable insight (even if it's "consider implementing X with Y data"), which indicates success.
- **Scoring Mechanism**: Each criterion can be scored 0 or 1 (or 0.5 for partial) and then aggregated. A simple approach is to treat each as equal weight (20% each if five criteria). For instance:
- If all criteria are met, usefulness = 100%.
- If one criterion wasn't met (say prerequisites were not mentioned at all), that would be 4/5 = 80%. We can convert that to a percentage or keep as fraction. In some cases, we might weight them slightly differently (perhaps mapping pain to archetype is the core function, so it could have a higher weight). But equal weighting keeps it straightforward.
- **Detection of Criteria in Conversation**: The system uses both **heuristics and signals from the agent** to mark these:
- _Ambiguity resolution:_ The agent can set a flag when it asks a clarification question. For example, if Stage 1 couldn't confidently extract the intent and the agent had to ask something like "Could you clarify which department this is about?", we consider ambiguity resolution in play. Once the user answers and the agent proceeds, we count that as a success (since the ambiguity got resolved through interaction). If no clarification was needed (clear input), we can also mark this as N/A or fulfilled by default.
- _Pain → archetype:_ During Stage 3, the agent will have likely chosen an archetype or solution category. We can detect this by checking which archetype nodes were heavily referenced or if the final recommendation matches one from our knowledge base. For example, if the final answer mentions "forecasting model" or "optimize schedule", we map that to an archetype like "Time-Series Forecasting" or "Optimization & Scheduling" and see if that archetype aligns with the pain. We have an internal mapping of pain types to archetypes (as in the graph). We simply verify that the pair is one of the expected mappings. If yes, score = 1 for this criterion. (If the assistant bizarrely recommended something unrelated, that'd be 0.)
- _Prerequisites:_ We can scan the final answer (or the reasoning logs) for presence of prerequisite-related content. For instance, did the assistant mention data requirements, or needed experts, or any checklist? If the final answer has a section or sentences like "Ensure you have X data and involve Y specialists," that's a clear positive. If not, but earlier in the log the agent considered prerequisites but maybe the user stopped before final answer, we could also consider that (though mostly we score based on what was communicated to user). We also have the knowledge of which prerequisites would be expected for the recommended archetype (via our graph), so we could double-check if those were mentioned. If at least one key prerequisite was discussed, we give credit.
- _Maturity alignment:_ We know the user's maturity (from Stage 1). We also have an idea of the complexity of the recommended solution (could be derived from the archetype's "analytical_purpose" or technical family - e.g., if it's a "Prescriptive" advanced technique vs a "Descriptive" basic one[\[23\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20complex%20deep%20learning)). We create a simple rule set: for each maturity level, which solution types are acceptable. For example:
  - If user is "Exploring", solutions marked as "Generative" or "Complex Prescriptive" might be considered misaligned (score 0 if recommended).
  - If user is "Scaling/Operationalized", any solution is fine (it's actually a plus if it's innovative). We then check the category of the recommended solution. If it falls within the allowed range for that maturity, we score 1. If it clearly overshoots the maturity, score 0 (with perhaps a warning flag).
- _Proposal quality:_ We can check that the final message contains at least one **specific** recommendation (the presence of a project name or a clear action verb) and isn't just generic. For example, "You should improve your data" vs "Implement a demand forecasting model to predict inventory" - the latter is specific. Heuristics: presence of terms like "implement", "develop", "project", or a name of a solution indicates concreteness. The absence of any of these or extremely high-level advice might fail this criterion.
- Many of these checks can be done with simple keyword searches or using the internal state of the agent (since the agent itself "knows" what it decided). We can also leverage that the chain-of-thought is logged: we could have the agent self-report these metrics in a structured way. However, initially we'll do it with straightforward code checks on the conversation content.
- **Metric Aggregation & Display**: Once we have the individual flags/scores, we compute the overall usefulness percentage. We then update the Streamlit metrics panel accordingly (as described in the Frontend section). For transparency, we might also display which criteria were met (as checkmarks). This effectively acts as a quick **"self-evaluation"** by the assistant at the end of each interaction.
- **Purpose of Evaluation**: The primary goal is to track whether the assistant is doing what it's supposed to. It's a form of internal QA. By marking these points in the conversation, we can identify if, for example, the assistant often forgets to discuss prerequisites - which would then highlight an area for improvement (prompt tuning or chain adjustment). It's also potentially visible to the user as feedback (e.g., "assistant addressed my problem well" vs "assistant didn't provide actionable insight"). In an enterprise setting, one might use such metrics to measure conversation success rates across many sessions.
- **Relation to Known Metrics**: This approach aligns with the idea of task-specific success metrics and goal completion rates[\[24\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=,written%20text). Instead of generic "BLEU scores" or open-ended user ratings, we have concrete success criteria for the task at hand. Each conversation is essentially a task (produce a viable project proposal). We measure if that task was completed satisfactorily. A 100% score corresponds to full completion of all sub-tasks (clarify needs, propose relevant solution, cover prerequisites, etc.) which is analogous to a perfect _task success_. Partial scores indicate which parts of the task were missed. This is much more actionable than a single perplexity or user satisfaction score, because we know _which_ part failed.
- **Future Enhancements**: In future iterations, we could incorporate more advanced eval methods:
- Using an LLM-as-a-judge approach where an AI model also provides a second opinion on the conversation quality (but that might cost API calls).
- Collecting user feedback at the end ("Did you find this useful?") to correlate with our internal score.
- But for the MVP, the rule-based rubric is transparent and cost-free, and directly tied to the design objectives.

## Deployment Strategy

The system is designed to be deployable on GCP using mostly managed services and free-tier options, ensuring both scalability and cost-efficiency. Below is the deployment plan and considerations:

- **Containerization and Cloud Run**: The backend (which in our case includes the Streamlit app and all Python logic) is containerized into a Docker image. We will use a lightweight base (e.g., Python 3.x slim image) and install necessary Python libraries (LangChain, streamlit, networkx, etc.) in the container. This container is then deployed to **Cloud Run**, GCP's serverless container platform. Cloud Run was chosen because:
- It can easily run Streamlit as a web service (just listening on a port). We simply configure the Docker entrypoint to run streamlit run app.py --server.port \$PORT.
- It scales down to zero when not in use and can scale up if multiple users access it, all within the limits we set. Importantly, the free tier for Cloud Run covers 2 million requests per month, and a generous amount of CPU and memory time[\[25\]](https://cloud.google.com/run/pricing#:~:text=Free%20tier%20%28based%20on%20us,active%20pricing), which is more than enough for a demo with moderate usage.
- We will ensure the container's memory/CPU requests are modest (Streamlit and our models are not too heavy by themselves) to stay within free limits. For instance, perhaps 1 vCPU and 1-2 GB RAM allocation.
- Cloud Run also handles HTTPS endpoints out of the box, making our app accessible securely via a URL.
- **Vertex AI Integration**: We utilize **Vertex AI** for the heavy ML tasks:
- _Embedding API_: The app calls Vertex's text embedding model for knowledge chunk embedding. These calls are relatively fast (each ~768-d vector generation) and can be done at startup (embedding all chunks) or on-demand if we add new knowledge. We will need to authenticate these calls (likely by using the Google Cloud service account credentials that the Cloud Run service has, or by providing an API key). Because we're within GCP, authentication can be seamless via Application Default Credentials.
- _LLM API (Text generation)_: We call Vertex's Generative AI (e.g., PaLM 2 text-bison or the upcoming Gemini model) for the chat agent's prompts. Specifically, Stage 1, Stage 3, and Stage 4 involve LLM calls. Vertex offers these via the PaLM API. We will use the Vertex AI SDK or direct REST calls from our Python code. The prompts and context are assembled by our LangChain agent and then sent to Vertex model endpoint (for example, model name "projects/\*/locations/us-central1/publishers/google/models/text-bison@001" or similar).
- _Cost considerations_: Vertex's generative model calls are billed per text token. We will use the smallest model that achieves good results (perhaps text-bison-001 or chat-bison) to minimize cost. Also, Cloud Run allows us to set a concurrency >1 (handle several chats in one container) which combined with the fast inference from Vertex ensures even the free \$300 trial credits would last for many demo sessions. The design assumes some budget for model calls, but given moderate usage and possibly caching of embedding results, this should stay low. In a demo environment, we can also set quotas to prevent excessive use.
- **Firebase for Persistence**: We integrate **Firebase Firestore** as a managed NoSQL database to store user session data. The idea is to enable a user to come back to their session via a unique URL or an email-linked login:
- When a session starts (or when the user provides their email), we create a _session document_ in Firestore. This could contain a summary of the conversation or important state (like the identified user context and the final recommendations given).
- We only store **summary** information to stay within free tier and also to avoid storing potentially large conversation logs (which could exceed free writes if done for every message). For example, we might store: email: <user@example.com>, last_active: timestamp, intent: {function: X, pain: Y, maturity: Z}, recommendation: "Optimization project", score: 80%. Basically, what was the outcome of their last session.
- We can then generate a unique link (perhaps the document ID or a short code) that the user can bookmark. Next time they hit that link, the app will load the summary from Firestore and greet them like, "Welcome back! Last time, we discussed an optimization project. Would you like to continue or start new?"
- Firestore's free tier allows 1 GiB storage and 20K writes/50K reads per day[\[26\]](https://cloud.google.com/firestore/pricing#:~:text=Quota), which is plenty for storing small JSON docs for each session for an initial user base. Even if we stored full conversation transcripts (which we likely won't), those numbers are sufficient for a pilot.
- Firebase also gives us the option to use Authentication (if we wanted user login) and hosting, but for now we only need Firestore and we identify sessions via a shareable link or email (not a secure auth, but okay for this use-case).
- **Why persistence?** This feature is a value-add to allow users (especially decision-makers) to pick up where they left off. It also showcases that the assistant can maintain continuity beyond a single chat lifespan. Technically, this means if the Cloud Run container went down, we don't lose all info - key outputs live in Firestore.
- **Optional Local Deployment**: While GCP is the primary target, the system can also run locally for development or demo in environments without internet:
- We abstract our calls so that if Vertex AI is not available, we could swap in local alternatives (e.g., using HuggingFace transformers for a local LLM and embedding model, and a local SQLite for persistence or just in-memory).
- Streamlit makes local running easy (just streamlit run app.py). The main difference is that the user would access it on localhost, and we might not enable email persistence (or could write to a local file).
- This dual capability (cloud vs local) is facilitated by configuration flags. For instance, a config might specify MODE = "GCP" or "LOCAL". In GCP mode, use Vertex and Firebase; in LOCAL, use local models and skip persistence.
- This flexibility is useful for testing without incurring API costs and demonstrates that we are not locked into a single vendor if not needed.
- **Security and Keys**: We will keep any API keys or service account credentials out of the code (using environment variables or GCP Secret Manager). Cloud Run can be configured with necessary IAM permissions so that the code can call Vertex and Firestore without embedding secrets (using the Workload Identity). For Pinecone (if used), we'd store that key in an env var in Cloud Run.
- **Deployment Process**: Using GCP's build tools:
- Build the Docker image (e.g., using Cloud Build or a local build push to Container Registry/Artifact Registry).
- Deploy to Cloud Run: gcloud run deploy assistant-chatbot --image=gcr.io/PROJECT/assistant:latest --allow-unauthenticated --memory=2Gi --region=us-central1 (with appropriate settings).
- Set environment variables for API keys (if any) in Cloud Run configuration. Also set the concurrency to maybe 1 or few if the LLM calls are heavy (to avoid one container handling too many at once).
- Once deployed, access the service URL which will load the Streamlit UI.
- We will have also set up Firestore in the same GCP project (which requires enabling Firestore and setting up in test mode or with proper rules).
- **Free Tier Utilization**: The entire stack is chosen to **maximize free-tier usage**:
- Cloud Run: first 2 million requests and considerable compute time free[\[25\]](https://cloud.google.com/run/pricing#:~:text=Free%20tier%20%28based%20on%20us,active%20pricing) (we likely stay well under this).
- Firebase: free Spark plan covers our minimal Firestore usage[\[26\]](https://cloud.google.com/firestore/pricing#:~:text=Quota).
- Vertex AI: not exactly free, but new GCP accounts have \$300 credit. Also, Vertex's lower-tier models cost fractions of cents per call (e.g., ~\$0.002 per text request depending on length). For a demo, this is negligible. We can also restrict the model usage (like limit to 1000 tokens per response, etc., to control cost).
- Alternatively, we could have used open-source models on a local GPU to avoid Vertex cost, but that would violate the "prefer GCP services" goal and also complicate Cloud Run deployment (Cloud Run doesn't support GPUs in free tier). Thus, Vertex is a reasonable choice given the small scale.
- Pinecone free tier (if we use it) similarly costs \$0 for our scale[\[7\]](https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df#:~:text=,to%20a%20more%20scalable%20solution).
- In sum, the architecture can be run almost entirely free for moderate usage, which is ideal for an MVP/portfolio project or initial pilot in a company.

Finally, this deployment strategy ensures the system is **portable and scalable**. All components (Streamlit, LangChain, etc.) run in a stateless container (state like user session is either ephemeral in memory or stored in Firebase). This means we could scale out with multiple replicas on Cloud Run easily if needed. GCP's managed services handle the heavy lifting (no managing VM servers, vector DB servers, etc.). It also showcases a modern cloud-native approach: stateless web app + managed AI APIs + serverless data store.

## Technology Stack

The solution leverages a range of technologies, focusing on Python-based frameworks and GCP services that have a free or low-cost tier. Below is a summary of the tech stack components:

- **Programming Language**: Python - the core language for implementation (agent logic, data processing, and the Streamlit app). Python's rich ecosystem for ML and data and its support in GCP (Vertex AI, etc.) make it an optimal choice.
- **AI & ML Libraries**:
- **LangChain** - used for orchestrating the LLM calls, implementing the ReAct agent, and managing the chain-of-thought prompting. It provides abstractions for tools and memory that we utilize for the multi-stage reasoning[\[10\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Force%20explicit%20mapping).
- **NetworkX** - used to construct and traverse the knowledge graph in memory[\[4\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=Subsequently%2C%20we%20construct%20the%20Knowledge,to%20form%20a%20comprehensive%20representation). It handles nodes/edges and properties with ease in Python.
- **FAISS** - (Facebook AI Similarity Search) used as the in-memory vector store for embedding-based retrieval. It's an efficient C++/Python library for similarity search, ideal for our semantic search needs.
- _(Optional)_ **Pinecone** - a managed vector database service. We might use the Pinecone Python client to store/retrieve vectors if persistent storage is needed or to demonstrate cloud vector DB usage. Only free-tier limits are used[\[7\]](https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df#:~:text=,to%20a%20more%20scalable%20solution).
- **Google Cloud Vertex AI SDK** - to call embedding and LLM models. The SDK (or REST calls) allows us to use models like text-bison (for generation) and embedding-gecko (for embeddings) easily. This ties into GCP's ML infrastructure (with likely low latency and high reliability).
- **Firebase Admin SDK (Python)** - for storing and retrieving session data in Firestore. This provides a simple way to interact with Firebase from Python, abstracting away REST calls.
- **Frontend & UI**:
- **Streamlit** - a Python web app framework for data apps. It lets us create the chat interface and log panel declaratively and handles the web server aspect. We use Streamlit components for text boxes, markdown display, expanders, etc., to build the UI quickly.
- Basic **HTML/CSS in Markdown** - to do custom formatting in the log panel (Streamlit allows limited HTML or emojis in markdown to color-code or style text).
- **Cloud Infrastructure**:
- **Docker** - containerizing the application for consistent deployment.
- **GCP Cloud Run** - hosting the containerized app serverlessly. It provides auto-scaling, HTTPS, and a generous free execution tier[\[25\]](https://cloud.google.com/run/pricing#:~:text=Free%20tier%20%28based%20on%20us,active%20pricing).
- **GCP Vertex AI** - providing the large language model (for the agent's reasoning and responses) and the embedding model for vectorization. Models like PaLM 2 (text-bison) and Vertex Embeddings are used via API calls.
- **Firebase (Firestore)** - acting as the persistence layer (NoSQL database) for session storage. We use the free Spark plan which includes Firestore with the quotas we need[\[26\]](https://cloud.google.com/firestore/pricing#:~:text=Quota).
- **Google Cloud Logging** - (optional in MVP) could capture logs if we extended observability beyond the UI. Not explicitly used in the current design, but Cloud Run automatically logs stdout/stderr which can be viewed in Cloud Logging console for debugging.
- **Data & Knowledge Base**:
- **JSON files** - (AI_archetypes.json, AI_prerequisites.json, AI_discovery.json) containing the curated domain knowledge. These are loaded at startup into the knowledge graph. JSON was chosen for its readability and ease of editing (for adding more archetypes or updating descriptions without code changes).
- **Development & Testing**:
- **Jupyter Notebooks** (for initial prototyping) - Possibly used to experiment with prompts, test the knowledge graph construction, and try out LangChain agent flows interactively.
- **GitHub** - for version control of the code and configuration (if this is a portfolio project, the spec and code would likely be stored there).
- **CI/CD** - not strictly necessary for MVP, but Cloud Build triggers or GitHub Actions could be set up to auto-deploy to Cloud Run on new commits.

This stack is highly **modular** and **extensible**. Each part (graph, vector store, LLM, UI) could be swapped out if needed (for example, using an OpenAI API instead of Vertex, or using a different frontend framework) with minimal changes to the overall architecture. We focused on open-source libraries and managed services that allow rapid development without infrastructure maintenance.

Importantly, all chosen components have **free tiers or open-source licenses**, aligning with the "free-tier friendly" requirement. For instance, Firebase and Cloud Run free quotas cover our needs, LangChain/NetworkX/FAISS are open-source (no cost), and even Pinecone and Vertex AI can be utilized at low volume within free allowances or free trial credits[\[7\]](https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df#:~:text=,to%20a%20more%20scalable%20solution)[\[25\]](https://cloud.google.com/run/pricing#:~:text=Free%20tier%20%28based%20on%20us,active%20pricing). This means we can demonstrate a fully functional, cloud-deployed AI assistant with **minimal upfront cost**.

In conclusion, the Prescriptive Analytical Assistant's technical architecture is a blend of _knowledge-driven AI_ and _cloud-native app design_. It utilizes a knowledge graph for structure, RAG for factual grounding, an LLM with ReAct for intelligent reasoning, and an intuitive interface for user interaction. The design emphasizes modularity, transparency, and feasibility (by leveraging GCP's free resources and Python's ecosystem) - making it both a credible technical solution for senior engineers and a compelling, working demonstration for evaluators and portfolio reviewers. All claims and design choices have been justified with best practices and, where possible, grounded in references to ensure the specification is technically sound and up-to-date.[\[11\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C,Multi%E2%80%91hop%20reasoning%20%26%20agentic)[\[24\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=,written%20text)

[\[1\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=When%20combined%20with%20knowledge%20graphs%2C,specific%20queries) [\[4\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=Subsequently%2C%20we%20construct%20the%20Knowledge,to%20form%20a%20comprehensive%20representation) [\[8\]](https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685#:~:text=2) Graph RAG Demystified. The blog uses Networkx library from… | by Soumya Chak | Medium

<https://medium.com/@soumya.chak3/graph-rag-demystified-f73556c65685>

[\[2\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Include) [\[3\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20complex%20deep%20learning) [\[9\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,Final%20formatted%20report%2C%20checklist) [\[10\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Force%20explicit%20mapping) [\[11\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C,Multi%E2%80%91hop%20reasoning%20%26%20agentic) [\[12\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,report%2C%20checklist%2C%20evaluation%20form) [\[13\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=%7C%20,) [\[15\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,data%20sources%2C%20deduplicate%20retrieval%2C%20and) [\[16\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=feasibility%20for%20high%E2%80%91growth%20organizations.%E2%80%9D%20,claims%20to%20the%20structured%20store) [\[17\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,claims%20to%20the%20structured%20store) [\[18\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=1,Transparency%20%26%20Precision) [\[19\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20Y%3B%20evidence%20and%20thresholds) [\[22\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,Agentic) [\[23\]](file://file-TnFueo1hMTdXGrpFV2R4At#:~:text=,over%20complex%20deep%20learning) README.md

file://file-TnFueo1hMTdXGrpFV2R4At

[\[5\]](file://file-NmJA3Jy2vq4bTEeF5cFCDL#:~:text=,Decision%20Tree) [\[6\]](file://file-NmJA3Jy2vq4bTEeF5cFCDL#:~:text=,Market%20basket%20rules) AI_prerequisites.json

file://file-NmJA3Jy2vq4bTEeF5cFCDL

[\[7\]](https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df#:~:text=,to%20a%20more%20scalable%20solution) Which Vector Database is Right for Your Generative AI Application? Pinecone vs ChromaDB | by JaikarSakhamuri | Medium

<https://medium.com/@sakhamurijaikar/which-vector-database-is-right-for-your-generative-ai-application-pinecone-vs-chromadb-1d849dd5e9df>

[\[14\]](https://docs.langchain.com/oss/python/langchain/agents#:~:text=Tool%20use%20in%20the%20ReAct,loop) Agents - Docs by LangChain

<https://docs.langchain.com/oss/python/langchain/agents>

[\[20\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=ethical%20considerations) [\[21\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=Map%20out%20every%20potential%20step,step%20problem) [\[24\]](https://www.ibm.com/think/topics/ai-agent-evaluation#:~:text=,written%20text) What is AI Agent Evaluation? | IBM

<https://www.ibm.com/think/topics/ai-agent-evaluation>

[\[25\]](https://cloud.google.com/run/pricing#:~:text=Free%20tier%20%28based%20on%20us,active%20pricing) Cloud Run pricing | Google Cloud

<https://cloud.google.com/run/pricing>

[\[26\]](https://cloud.google.com/firestore/pricing#:~:text=Quota) Firestore pricing | Google Cloud

<https://cloud.google.com/firestore/pricing>
